buildscript {
    repositories {
        mavenCentral()
     }

    dependencies {
        classpath 'org.kt3k.gradle.plugin:coveralls-gradle-plugin:2.4.0'
    }
}

apply plugin: 'java'
apply plugin: 'idea'
apply plugin: 'application'
apply plugin: "jacoco"
apply plugin: 'com.github.kt3k.coveralls'

mainClassName = "org.broadinstitute.hellbender.Main"

repositories {
    mavenCentral()
    jcenter()
    maven {
        url "https://repository.cloudera.com/artifactory/cloudera-repos/" // spark-dataflow
    }
    maven {
        url "https://oss.sonatype.org/content/repositories/snapshots/" // for http-client
    }
    maven {
        url "https://oss.sonatype.org/content/groups/public/" //for hellbender (gatk4)
    }

}

configurations.all {
    resolutionStrategy {
        force 'com.google.http-client:google-http-client:1.21.0-SNAPSHOT'
        // the snapshot folder contains a dev version of guava, we don't want to use that.
        force 'com.google.guava:guava:18.0'
    }
    all*.exclude group: 'com.google.guava', module:'guava-jdk5'
}

jacocoTestReport {
    dependsOn test
    group = "Reporting"
    description = "Generate Jacoco coverage reports after running tests."
    additionalSourceDirs = files(sourceSets.main.allJava.srcDirs)

    reports {
        xml.enabled = true // coveralls plugin depends on xml format report
        html.enabled = true
    }
}

jacoco {
    toolVersion = "0.7.1.201405082137"
}

//NOTE: we ignore contracts for now
compileJava {
  options.compilerArgs = ['-proc:none', '-Xlint:all','-Werror']
}
compileTestJava {
  options.compilerArgs = ['-proc:none', '-Xlint:all','-Werror']
}

build.dependsOn installDist
check.dependsOn installDist

dependencies {
    compile files("${System.properties['java.home']}/../lib/tools.jar")

    compile 'org.broadinstitute:hellbender:4.pre-alpha-1-gd612727-SNAPSHOT'
    compile 'com.google.guava:guava:18.0'
    compile 'com.github.samtools:htsjdk:1.140'
    compile ('com.google.cloud.genomics:google-genomics-dataflow:v1beta2-0.15') {
        // an upstream dependency includes guava-jdk5, but we want the newer version instead.
        exclude module: 'guava-jdk5'
        exclude group:  'guava-jdk5'
    }
    compile ('com.google.cloud.genomics:gatk-tools-java:1.1') {
        exclude module: 'guava-jdk5'
    }
    compile 'org.apache.logging.log4j:log4j-api:2.3'
    compile 'org.apache.logging.log4j:log4j-core:2.3'
    compile 'org.apache.commons:commons-vfs2:2.0'
    compile 'org.reflections:reflections:0.9.10'
    compile 'com.google.cloud.dataflow:google-cloud-dataflow-java-sdk-all:0.4.150727'
    compile ('com.google.apis:google-api-services-genomics:v1beta2-rev56-1.20.0') {
        exclude module: 'guava-jdk5'
    }
    compile ('com.google.cloud.genomics:google-genomics-utils:v1beta2-0.30') {
        exclude module: 'guava-jdk5'
    }


    // we don't need this guy. Plus, it asks for the latest version
    // of com.google.http-client:google-http-client-jackson2,
    // which is a problem when we have a SNAPSHOT version.
    //compile 'com.google.appengine.tools:appengine-gcs-client:0.4.4'
    compile 'org.jgrapht:jgrapht-core:0.9.1'
    compile 'org.testng:testng:6.9.6' //compile instead of testCompile because it is needed for test infrastructure that needs to be packaged
    compile 'com.cloudera.dataflow.spark:spark-dataflow:0.4.0.1'

    compile('org.apache.hadoop:hadoop-client:2.2.0') // should be a 'provided' dependency

    compile('org.apache.spark:spark-core_2.10:1.4.1') {
        // JUL is used by Google Dataflow as the backend logger, so exclude jul-to-slf4j to avoid a loop
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
    }

    compile('de.javakaffee:kryo-serializers:0.26') {
        exclude module: 'kryo' // use Spark's version
    }

    //needed for DataflowAssert
    testCompile 'org.hamcrest:hamcrest-all:1.3'
    testCompile 'junit:junit:4.12'
    testCompile "org.mockito:mockito-core:1.10.19"
    testCompile 'org.apache.hadoop:hadoop-minicluster:2.7.1'
}

sourceCompatibility = 1.8
targetCompatibility = 1.8

def String deriveVersion(){
    def stdout = new ByteArrayOutputStream()
    try {
        logger.info("path is $System.env.PATH")
        exec {
            commandLine "git", "describe", "--always"
            standardOutput = stdout;

            ignoreExitValue = true
        }
    } catch (GradleException e) {
        logger.error("Couldn't determine version.  " + e.getMessage())
    }
    return stdout.size() > 0 ? stdout.toString().trim() : "version-unknown"
}
final SNAPSHOT = "-SNAPSHOT"
version = deriveVersion() + SNAPSHOT  //all builds are snapshot builds until we decide that there is something we want to keep
boolean isRelease = ! version.endsWith(SNAPSHOT)
logger.info("build for version:" + version)
group = 'org.broadinstitute'


jar {
    manifest {
        attributes 'Implementation-Title': 'Hellbender-dataflow-tools',
                'Implementation-Version': version,
                'Main-Class': 'org.broadinstitute.hellbender.Main'
    }
}


task testAll(type: Test){
    useTestNG{}

    // ensure dataflowRunner is passed to the test VM if specified on the command line
    systemProperty "dataflowRunner", System.getProperty("dataflowRunner")
    // increase max buffer size for Spark serialization
    systemProperty "spark.kryoserializer.buffer.max", "256m"

    // set heap size for the test JVM(s)
    minHeapSize = "1G"
    maxHeapSize = "2G"

    // show standard out and standard error of the test JVM(s) on the console
    testLogging.showStandardStreams = true
    beforeTest { descriptor ->
        logger.lifecycle("Running Test: " + descriptor)
    }

    // listen to standard out and standard error of the test JVM(s)
    onOutput { descriptor, event ->
        logger.lifecycle("Test: " + descriptor + " produced standard out/err: " + event.message )
    }
}

test {
    String CI = "$System.env.CI"
    String CLOUD = "$System.env.CLOUD"
    String SPARK = "$System.env.SPARK"
    useTestNG{
        if (CLOUD =="mandatory") {
            // run only the cloud tests
            includeGroups 'cloud', 'bucket'
        } else if (CLOUD == "todo") {
            // run only the in-development cloud tests
            includeGroups 'cloud_todo', 'bucket_todo'
        } else if (CLOUD == "together") {
            // run both local tests and mandatory cloud tests, together.
            // This is good when e.g. you are done for the day and want to run tests on your machine overnight.
            excludeGroups 'cloud_todo', 'bucket_todo'
        } else {
            // run only the local tests
            excludeGroups 'cloud', 'bucket', 'cloud_todo', 'bucket_todo'
        }
    }

    // ensure dataflowRunner is passed to the test VM if specified on the command line
    systemProperty "dataflowRunner", System.getProperty("dataflowRunner")

    // set heap size for the test JVM(s)
    minHeapSize = "1G"
    maxHeapSize = "2G"

    if (CI == "true" && CLOUD == "false" ) {
        int count = 0
        // listen to events in the test execution lifecycle
        testLogging {
            events "skipped", "failed"
            exceptionFormat = "full"
        }

        beforeTest { descriptor ->
            count++
            if( count % 10000 == 0) {
                logger.lifecycle("Finished "+ Integer.toString(count++) + " tests")
            }
        }
    } else {
        // show standard out and standard error of the test JVM(s) on the console
        testLogging.showStandardStreams = true
        beforeTest { descriptor ->
            logger.lifecycle("Running Test: " + descriptor)
        }

        // listen to standard out and standard error of the test JVM(s)
        onOutput { descriptor, event ->
            logger.lifecycle("Test: " + descriptor + " produced standard out/err: " + event.message )
        }
    }
}

task wrapper(type: Wrapper) {
    gradleVersion = '2.7'
}


task fatJar(type: Jar) {
  manifest {
        attributes 'Implementation-Title': 'Hellbender',
          'Implementation-Version': version,
          'Main-Class': 'org.broadinstitute.hellbender.Main'
    }
    baseName = project.name + '-all'
    zip64 true
    from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } }
    with jar
}
